# -*- coding: utf-8 -*-
"""B20EE044_B20EE015_Course_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J5_juEVxPuf3pVLT2L8nNG3TYhBlJMYC
"""

!pip install xgboost

import pandas as pd
import numpy as np
import xgboost
from sklearn.model_selection import train_test_split as tts
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score as ac
from sklearn.tree import DecisionTreeClassifier as dtc
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
import statistics
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import confusion_matrix
from sklearn.ensemble import RandomForestClassifier as rfc
import lightgbm
from sklearn.metrics import classification_report
from sklearn.metrics import f1_score
from sklearn.preprocessing import label_binarize
from sklearn.metrics import roc_curve, auc
from sklearn.neural_network import MLPClassifier
from tabulate import tabulate

df1 = pd.read_csv('/content/features_3_sec.csv')

df1.shape

df1.head()

df = df1.drop(['filename', 'length'], axis=1)

df.head()

print(df.isnull().sum())

df.dtypes

df.tail()

x = df.iloc[:, 0:57]
y = df.iloc[:, 57]

x.head()

y.head()

x = x.to_numpy()
y = y.to_numpy()

le = LabelEncoder()

y = le.fit_transform(y)

x_train, x_test, y_train, y_test = tts(x, y, train_size = 0.8, random_state = 675)

ss = StandardScaler()

x1_train = ss.fit_transform(x_train)
x1_test = ss.fit_transform(x_test)

x3_train = x1_train
x3_test = x1_test

pca = PCA(n_components = 57)
x2_train = pca.fit_transform(x3_train)
x2_test = pca.fit_transform(x3_test)

table = []
l0 = ['Model', 'Simple model', 'With PCA', 'With LDA']
table.append(l0)

"""#**1.Lightgbm**"""

lgbm1 = lightgbm.LGBMClassifier()

lgbm1.fit(x_train, y_train)
lg_y1 = lgbm1.predict(x_test)
print(ac(y_test, lg_y1))

print(confusion_matrix(y_test, lg_y1))

print(classification_report(y_test, lg_y1))

"""LDA"""

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
lda = LDA(n_components = 2)
lda.fit(x_train,y_train)
x_train2= lda.transform(x_train)
x_test2= lda.transform(x_test)

model1 = lightgbm.LGBMClassifier()
model1.fit(x_train2, y_train)
y_pred1 = model1.predict(x_test2)

accu1=ac(y_test,y_pred1)
print(accu1)

"""**ROC**"""

w1 = lgbm1.predict_proba(x_test)
y_bin = label_binarize(y_test, classes = np.unique(y_test))
fpr = {}
tpr = {}
thresh = {}
roc_auc = dict()

label = np.unique(y_test)
n_classes = label.shape[0]
for i in range(n_classes):
  fpr[i], tpr[i], thresh[i] = roc_curve(y_bin[:, i], w1[:, i])
  roc_auc[i] = auc(fpr[i], tpr[i])
  plt.plot(fpr[i], tpr[i], linestyle = '--', label = '%s vs rest (AUC = %0.2f)' % (label[i], roc_auc[i]))

plt.plot([0, 1], [0, 1], 'b--')
plt.xlim([0, 1])
plt.ylim([0, 1.05])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')

"""PCA"""

P1 = lightgbm.LGBMClassifier()
P1.fit(x2_train, y_train)

Q1 = P1.predict(x2_test)

print(ac(y_test, Q1))

l1 = ['Lightgbm Classfier', ac(y_test, lg_y1),ac(y_test, Q1), ac(y_test,y_pred1)]
table.append(l1)

"""#**2.XGBClassiifier**"""

xgb1 = xgboost.XGBClassifier()
xgb1.fit(x_train, y_train)
xg_y1 = xgb1.predict(x_test)

ac1 = ac(y_test, xg_y1)

print(ac1)

print(confusion_matrix(y_test, xg_y1))

print(classification_report(y_test, xg_y1))

"""**ROC**"""

w2 = xgb1.predict_proba(x_test)
y_bin = label_binarize(y_test, classes = np.unique(y_test))
fpr = {}
tpr = {}
thresh = {}
roc_auc = dict()

label = np.unique(y_test)
n_classes = label.shape[0]
for i in range(n_classes):
  fpr[i], tpr[i], thresh[i] = roc_curve(y_bin[:, i], w2[:, i])
  roc_auc[i] = auc(fpr[i], tpr[i])
  plt.plot(fpr[i], tpr[i], linestyle = '--', label = '%s vs rest (AUC = %0.2f)' % (label[i], roc_auc[i]))

plt.plot([0, 1], [0, 1], 'b--')
plt.xlim([0, 1])
plt.ylim([0, 1.05])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')

"""LDA"""

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
lda = LDA(n_components = 2)
lda.fit(x_train,y_train)
x_train3= lda.transform(x_train)
x_test3= lda.transform(x_test)

model2 = xgboost.XGBClassifier()
model2.fit(x_train3, y_train)
y_pred2 = model1.predict(x_test3)

accu2=ac(y_test,y_pred2)
print(accu2)

""" PCA"""

P2 = xgboost.XGBClassifier()
P2.fit(x2_train, y_train)

Q2 = P2.predict(x2_test)

print(ac(y_test, Q2))

l2 = ['Xgboost Classifier', ac(y_test, xg_y1), ac(y_test, Q2), ac(y_test,y_pred2)]
table.append(l2)

"""#**3.KNN Classificatiion**"""

n = []
accuracy = []
for i in range(1, 13):
  m2 = KNeighborsClassifier(n_neighbors = i)
  cv = KFold(n_splits=5, random_state=1, shuffle=True)
  scores = cross_val_score(m2, x1_train, y_train, scoring='accuracy', cv = cv, n_jobs= -1)
  n.append(i)
  accuracy.append(statistics.mean(abs(scores)))

plt.plot(n, accuracy)
plt.xlabel('n_neighbors')
plt.ylabel('CV Accuracy')
plt.show()

error_rate = []
n = []
for i in range(1, 13):
  m2 = KNeighborsClassifier(n_neighbors = i)
  m2.fit(x1_train, y_train)
  pred_i = m2.predict(x1_test)
  error_rate.append(np.mean(pred_i != y_test))
  n.append(i)

plt.plot(n, error_rate)
plt.xlabel('n_neighbors')
plt.ylabel('error_rate')
plt.show()

knn1 = KNeighborsClassifier(n_neighbors = 3)
knn1.fit(x1_train, y_train)
knn_y1 = knn1.predict(x1_test)

print(ac(y_test, knn_y1))

print(confusion_matrix(y_test, knn_y1))

print(classification_report(y_test, knn_y1))

"""**ROC**"""

w3 = knn1.predict_proba(x_test)
y_bin = label_binarize(y_test, classes = np.unique(y_test))
fpr = {}
tpr = {}
thresh = {}
roc_auc = dict()

label = np.unique(y_test)
n_classes = label.shape[0]
for i in range(n_classes):
  fpr[i], tpr[i], thresh[i] = roc_curve(y_bin[:, i], w3[:, i])
  roc_auc[i] = auc(fpr[i], tpr[i])
  plt.plot(fpr[i], tpr[i], linestyle = '--', label = '%s vs rest (AUC = %0.2f)' % (label[i], roc_auc[i]))

plt.plot([0, 1], [0, 1], 'b--')
plt.xlim([0, 1])
plt.ylim([0, 1.05])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')

"""LDA"""

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
lda = LDA(n_components = 2)
lda.fit(x_train,y_train)
x_train4= lda.transform(x_train)
x_test4= lda.transform(x_test)

model3 = KNeighborsClassifier()
model3.fit(x_train4, y_train)
y_pred3 = model1.predict(x_test4)

accu3=ac(y_test,y_pred3)
print(accu3)

"""PCA"""

P3 = KNeighborsClassifier()
P3.fit(x2_train, y_train)

Q3 = P3.predict(x2_test)

print(ac(y_test, Q3))

l3 = ['KNN Classifier', ac(y_test, knn_y1), ac(y_test, Q3), ac(y_test,y_pred3)]
table.append(l3)

"""#**4.Random Forest Classifier**"""

rfc1 = rfc()
rfc1.fit(x_train, y_train)
rfc_y1 = rfc1.predict(x_test)

print(ac(y_test, rfc_y1))

print(confusion_matrix(y_test, rfc_y1))

print(classification_report(y_test, rfc_y1))

w4 = rfc1.predict_proba(x_test)
y_bin = label_binarize(y_test, classes = np.unique(y_test))
fpr = {}
tpr = {}
thresh = {}
roc_auc = dict()

label = np.unique(y_test)
n_classes = label.shape[0]
for i in range(n_classes):
  fpr[i], tpr[i], thresh[i] = roc_curve(y_bin[:, i], w4[:, i])
  roc_auc[i] = auc(fpr[i], tpr[i])
  plt.plot(fpr[i], tpr[i], linestyle = '--', label = '%s vs rest (AUC = %0.2f)' % (label[i], roc_auc[i]))

plt.plot([0, 1], [0, 1], 'b--')
plt.xlim([0, 1])
plt.ylim([0, 1.05])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')

"""LDA"""

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
lda = LDA(n_components = 2)
lda.fit(x_train,y_train)
x_train5= lda.transform(x_train)
x_test5= lda.transform(x_test)

model4 = rfc()
model4.fit(x_train5, y_train)
y_pred4 = model4.predict(x_test5)
accu4=ac(y_test,y_pred4)
print(accu4)

"""PCA"""

P4 = rfc()
P4.fit(x2_train, y_train)

Q4 = P4.predict(x2_test)

print(ac(y_test, Q4))

l4 = ['Random Forest Classifier', ac(y_test, rfc_y1), ac(y_test, Q4), ac(y_test,y_pred4)]
table.append(l4)

"""#**5.Decision Tree**"""

dtc1 = dtc()
dtc1.fit(x_train, y_train)
dtc_y1 = dtc1.predict(x_test)

print(ac(y_test, dtc_y1))

print(confusion_matrix(y_test, dtc_y1))

print(classification_report(y_test, dtc_y1))

depth_range = [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
sample_len = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
para = {'max_depth' : depth_range, 'min_samples_leaf': sample_len}
m1 =  dtc()
gscv = GridSearchCV(m1, para, cv = 5)
gscv.fit(x_train, y_train)
gscv.best_params_

model2= dtc(max_depth= 15, min_samples_leaf= 2)
model2.fit(x_train, y_train)
y_pred2 = model2.predict(x_test)
accu2=ac(y_test,y_pred2)
print(accu2)

"""ROC"""

w5 = model2.predict_proba(x_test)
y_bin = label_binarize(y_test, classes = np.unique(y_test))
fpr = {}
tpr = {}
thresh = {}
roc_auc = dict()

label = np.unique(y_test)
n_classes = label.shape[0]
for i in range(n_classes):
  fpr[i], tpr[i], thresh[i] = roc_curve(y_bin[:, i], w5[:, i])
  roc_auc[i] = auc(fpr[i], tpr[i])
  plt.plot(fpr[i], tpr[i], linestyle = '--', label = '%s vs rest (AUC = %0.2f)' % (label[i], roc_auc[i]))

plt.plot([0, 1], [0, 1], 'b--')
plt.xlim([0, 1])
plt.ylim([0, 1.05])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')

"""**LDA**"""

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
lda = LDA(n_components = 2)
lda.fit(x_train,y_train)
X_train_2= lda.transform(x_train)
X_test_2= lda.transform(x_test)

model_4 = dtc()
model_4.fit(X_train_2, y_train)
y_pred_4 = model_4.predict(X_test_2)

accu_3=ac(y_test,y_pred_4)
print(accu_3)

"""PCA"""

P5 = dtc()
P5.fit(x2_train, y_train)

Q5 = P5.predict(x2_test)

print(ac(y_test, Q5))

l5 = ['Decision Tree Classifier', ac(y_test, dtc_y1), ac(y_test, Q5), ac(y_test,y_pred_4)]
table.append(l5)

"""#**6.MLP**"""

from sklearn.neural_network import MLPClassifier
import seaborn as sns
import matplotlib.pyplot as plt

mlp=MLPClassifier(max_iter=500, activation="relu")
mlp.fit(x1_train, y_train)

y_pred5 = mlp.predict(x1_test)

accu5=ac(y_test,y_pred5)
print(accu5)

confusion_matrix(y_test,y_pred5)

print(classification_report(y_test,y_pred5))

"""ROC"""

w6 = mlp.predict_proba(x_test)
y_bin = label_binarize(y_test, classes = np.unique(y_test))
fpr = {}
tpr = {}
thresh = {}
roc_auc = dict()

label = np.unique(y_test)
n_classes = label.shape[0]
for i in range(n_classes):
  fpr[i], tpr[i], thresh[i] = roc_curve(y_bin[:, i], w6[:, i])
  roc_auc[i] = auc(fpr[i], tpr[i])
  plt.plot(fpr[i], tpr[i], linestyle = '--', label = '%s vs rest (AUC = %0.2f)' % (label[i], roc_auc[i]))

plt.plot([0, 1], [0, 1], 'b--')
plt.xlim([0, 1])
plt.ylim([0, 1.05])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')

"""LDA"""

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
lda = LDA(n_components = 2)
lda.fit(x1_train,y_train)
X_train_3= lda.transform(x_train)
X_test_3= lda.transform(x_test)

model_6 = LDA()
model_6.fit(X_train_3, y_train)
y_pred_6 = model_6.predict(X_test_3)

accu_6=ac(y_test,y_pred_6)
print(accu_6)

"""PCA"""

P6 = MLPClassifier()
P6.fit(x2_train, y_train)

Q6 = P6.predict(x2_test)

print(ac(y_test, Q6))

l6 = ['MLP', ac(y_test,y_pred5), ac(y_test, Q6), ac(y_test,y_pred_6)]
table.append(l6)

"""**Accuracy on Testing data**"""

print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))